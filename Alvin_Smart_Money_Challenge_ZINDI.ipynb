{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06kkNxjIdgSW"
   },
   "outputs": [],
   "source": [
    "# Libs imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import FastText\n",
    "import spacy\n",
    "import re\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost import Pool\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.read_csv('extra_data.csv')['MERCHANT_NAME'], \n",
    "           pd.read_csv('train.csv')['MERCHANT_NAME'],\n",
    "           pd.read_csv('test.csv')['MERCHANT_NAME']]).to_csv('all_labels3.txt', index = False, header = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = 'all_labels3.txt'#\"items_unique_text.txt\"\n",
    "\n",
    "\n",
    "class FileLinesIter:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        with open(self.filename, 'r', encoding='utf-8') as fin:\n",
    "            for line in fin:\n",
    "                yield line.split()\n",
    "\n",
    "class ListLinesIter:\n",
    "    def __init__(self, goods_names):\n",
    "        self.goods_names = goods_names\n",
    "\n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.goods_names)\n",
    "        for good in self.goods_names:\n",
    "            yield good\n",
    "\n",
    "#### Make And Train FastText Model\n",
    "\n",
    "model_good = FastText(\n",
    "    vector_size=21,# (int, optional) – Dimensionality of the word vectors (embeddings).\n",
    "    window=3,       # (int, optional) – The maximum distance between the current and predicted word within a sentence.\n",
    "    min_count=1,    # (int, optional) – The model ignores all words with total frequency lower than this.\n",
    "    workers=1,#8,\n",
    "    seed = 1,\n",
    "    #negative=5,\n",
    "    #min_alpha=0.000001,\n",
    "    #max_vocab_size=500_000,\n",
    "    #bucket=1_000_000,\n",
    "    sg=1 # Sg = 1 -> skip-gram, \n",
    ")\n",
    "\n",
    "model_good.build_vocab(corpus_iterable=FileLinesIter(text_file))\n",
    "\n",
    "total_examples = model_good.corpus_count\n",
    "\n",
    "total_words = model_good.corpus_total_words\n",
    "\n",
    "model_good.train(\n",
    "    corpus_iterable=FileLinesIter(text_file), \n",
    "    total_examples=total_examples, \n",
    "    epochs = 20\n",
    ")\n",
    "\n",
    "model_fname = \"fast_text_best.ft\"\n",
    "\n",
    "model_good.save(model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657394251847,
     "user": {
      "displayName": "Roger Waters",
      "userId": "17117326896123090742"
     },
     "user_tz": -240
    },
    "id": "n9KORySPiN1r"
   },
   "outputs": [],
   "source": [
    "# Testing path\n",
    "# path = '/content/drive/MyDrive/Colab Notebooks/alvinapp/'\n",
    "path = \"\"\n",
    "\n",
    "# Load the files into a Pandas Dataframe\n",
    "train = pd.read_csv(path+'Train.csv')\n",
    "test = pd.read_csv(path+'Test.csv')\n",
    "extra = pd.read_csv(path+'extra_data.csv')\n",
    "ss = pd.read_csv(path+'SampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest challenge in this competition is the lack of labeled data. So we can use some unlabeled rows and label them by yourself using simple heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra.loc[extra['MERCHANT_NAME'].str.contains('PHARMACY'), 'MERCHANT_CATEGORIZED_AS'] = 'Health'\n",
    "\n",
    "extra.loc[extra['MERCHANT_NAME'].str.contains('CREDIT'), 'MERCHANT_CATEGORIZED_AS'] = 'Loan Repayment'\n",
    "\n",
    "extra.loc[extra['MERCHANT_NAME'].str.contains('GRILL'), 'MERCHANT_CATEGORIZED_AS'] = 'Going out'\n",
    "\n",
    "extra.loc[extra['MERCHANT_NAME'].str.contains('INSURANCE'),  'MERCHANT_CATEGORIZED_AS'] = 'Emergency fund'\n",
    "\n",
    "extra.loc[extra['MERCHANT_NAME'].str.contains('PIZZA'),  'MERCHANT_CATEGORIZED_AS'] = 'Going out'\n",
    "\n",
    "extra.loc[extra['MERCHANT_NAME'].str.contains('LOAN'),   'MERCHANT_CATEGORIZED_AS'] = 'Loan Repayment'\n",
    "\n",
    "extra.loc[extra['MERCHANT_NAME'].str.contains('ARTCAFFE'),  'MERCHANT_CATEGORIZED_AS'] = 'Going out'\n",
    "\n",
    "extra.loc[extra['MERCHANT_NAME'].str.contains('SAVINGS'),  'MERCHANT_CATEGORIZED_AS'] = 'Emergency fund'\n",
    "\n",
    "extra.loc[extra['MERCHANT_NAME'].str.contains('CASHNOW'), 'MERCHANT_CATEGORIZED_AS'] = 'Loan Repayment'\n",
    "\n",
    "extra.loc[extra['MERCHANT_NAME'].str.contains('GOOGLE'), 'MERCHANT_CATEGORIZED_AS'] = 'Bills & Fees'\n",
    "\n",
    "extra.loc[extra['MERCHANT_NAME'].str.contains('DECATHLON'), 'MERCHANT_CATEGORIZED_AS'] = 'Miscellaneous'\n",
    "\n",
    "extra.loc[extra['MERCHANT_NAME'].str.contains('KANDAMOJA APP'), 'MERCHANT_CATEGORIZED_AS'] = 'Loan Repayment'\n",
    "\n",
    "extra.loc[extra['MERCHANT_NAME'].str.contains('CLINIC'), 'MERCHANT_CATEGORIZED_AS'] = 'Health'\n",
    "\n",
    "extra.loc[(extra['MERCHANT_NAME'].str.contains('GAS')) & (extra['MERCHANT_NAME'].str.contains('STATION')), 'MERCHANT_CATEGORIZED_AS'] = 'Transport & Fuel'\n",
    "\n",
    "extra.loc[extra['MERCHANT_NAME'].str.contains('FUEL'), 'MERCHANT_CATEGORIZED_AS'] = 'Transport & Fuel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8AhJ0cKer7b"
   },
   "source": [
    "## 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HSj_1Qz7ezsw",
    "outputId": "46425755-d8be-4bab-9d03-4e491eb3162a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let’s observe the shape of our datasets.\n",
    "print('Train data shape :', train.shape)\n",
    "print('Test data shape :', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concating extra data labeled by hands\n",
    "train = pd.concat([train, extra.loc[extra['MERCHANT_CATEGORIZED_AS'].isna() == False]]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s observe the shape of our datasets.\n",
    "print('Train data shape :', train.shape)\n",
    "print('Test data shape :', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['MERCHANT_CATEGORIZED_AS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concstructing entity features using spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "train = pd.concat([train, pd.get_dummies(\n",
    "    train['MERCHANT_NAME'].apply(\n",
    "        lambda x: list(\n",
    "            set(\n",
    "                [x.label_ for x in nlp(x.lower()).ents]\n",
    "            )\n",
    "        )\n",
    "    ).explode()\n",
    ").sum(level = 0)], axis = 1)\n",
    "\n",
    "test = pd.concat([test, pd.get_dummies(\n",
    "    test['MERCHANT_NAME'].apply(\n",
    "        lambda x: list(\n",
    "            set(\n",
    "                [x.label_ for x in nlp(x.lower()).ents]\n",
    "            )\n",
    "        )\n",
    "    ).explode()\n",
    ").sum(level = 0)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping useless columns\n",
    "train.drop(columns = ['CARDINAL', 'GPE', 'ORG', 'PRODUCT'], inplace = True)\n",
    "test.drop(columns = ['CARDINAL', 'GPE', 'ORG', 'PRODUCT'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing Seasonal features\n",
    "\n",
    "def season_funct(x):\n",
    "    if x in [11, 12, 1, 2, 3]:\n",
    "        return 'DRY'\n",
    "    else:\n",
    "        return 'WET'\n",
    "\n",
    "train['season'] = train['PURCHASED_AT'].astype('datetime64').dt.month.apply(season_funct)\n",
    "test['season'] = test['PURCHASED_AT'].astype('datetime64').dt.month.apply(season_funct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTPRRb1dyH2R",
    "outputId": "51dff886-5657-468c-be0a-c955660ff7f7"
   },
   "outputs": [],
   "source": [
    "# Use a dictionary comprehension and zip to create a dictionary for all the categories in the train data\n",
    "labels_train = train['MERCHANT_CATEGORIZED_AS'].astype('category').cat.categories.tolist()\n",
    "replace_map_train = {'MERCHANT_CATEGORIZED_AS' : {k: v for k,v in zip(labels_train,list(range(1,len(labels_train)+1)))}}\n",
    "print(\"Train data: \", replace_map_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"USER_GENDER\"] = train[\"USER_GENDER\"].apply(lambda x: \"Male\" if pd.isna(x) else x)\n",
    "test[\"USER_GENDER\"] = test[\"USER_GENDER\"].apply(lambda x: \"Male\" if pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping useless columns\n",
    "train.drop(columns = ['USER_AGE'], inplace = True)\n",
    "test.drop(columns = ['USER_AGE'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, pd.get_dummies(train['USER_HOUSEHOLD'].apply(lambda x: 1 if x == 1 else 2 if x == 2 else 3 if x == 3 else 4 ), \n",
    "               prefix = 'USER_HOUSEHOLD')], axis = 1)\n",
    "\n",
    "test = pd.concat([test, pd.get_dummies(test['USER_HOUSEHOLD'].apply(lambda x: 1 if x == 1 else 2 if x == 2 else 3 if x == 3 else 4 ), \n",
    "               prefix = 'USER_HOUSEHOLD')], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns = ['USER_HOUSEHOLD'], inplace = True)\n",
    "test.drop(columns = ['USER_HOUSEHOLD'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wv = FastText.load(\"fast_text_best.ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing embeddings features\n",
    "train = pd.concat([train, pd.DataFrame(train['MERCHANT_NAME'].apply(lambda x: np.array([model_wv.wv[x] for x in x.split()]).mean(axis = 0)).to_list())], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([test, pd.DataFrame(test['MERCHANT_NAME'].apply(lambda x: np.array([model_wv.wv[x] for x in x.split()]).mean(axis = 0)).to_list())], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_cols = ['MERCHANT_CATEGORIZED_AT', 'MERCHANT_NAME', 'PURCHASED_AT', 'USER_ID', 'Transaction_ID']#, 'USER_HOUSEHOLD_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constricting frequent merchant names features \n",
    "s = train['MERCHANT_NAME'].apply(lambda x: re.sub('[^A-Za-z0-9]+', ' ', re.sub(' +',' ',x)).upper().split(' '))\n",
    "\n",
    "unique_words = s.apply(pd.Series).stack().reset_index(drop = True)\n",
    "\n",
    "vals = unique_words.value_counts().head(25).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in vals:\n",
    "    train[c] = train['MERCHANT_NAME'].str.contains(c).astype(int)\n",
    "    test[c] = test['MERCHANT_NAME'].str.contains(c).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(columns = [ x for x in to_drop_cols if x != 'Transaction_ID'], inplace = True)\n",
    "train.drop(columns = to_drop_cols, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing skeweness\n",
    "test['PURCHASE_VALUE'] = np.log1p(test['PURCHASE_VALUE'])\n",
    "train['PURCHASE_VALUE'] = np.log1p(train['PURCHASE_VALUE'])\n",
    "\n",
    "test['USER_INCOME'] = np.log1p(test['USER_INCOME'])\n",
    "train['USER_INCOME'] = np.log1p(train['USER_INCOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQ0GOPUdsJlI"
   },
   "outputs": [],
   "source": [
    "# Based on our data set, this function converts boolean to binary entries\n",
    "def create_binary_cols(content):\n",
    "  if content == False:\n",
    "    content = 0\n",
    "  elif content == True:\n",
    "    content = 1\n",
    "  elif content == 'N':\n",
    "    content = 0\n",
    "  elif content == 'Y':\n",
    "    content = 1\n",
    "  elif content == 'Male':\n",
    "    content = 0\n",
    "  elif content == 'Female':\n",
    "    content = 1\n",
    "  elif content == 'Unknown':\n",
    "    content = -1    \n",
    "  return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WV7llutosRsK"
   },
   "outputs": [],
   "source": [
    "# Gender column convert:\n",
    "train['USER_GENDER'] = train['USER_GENDER'].apply(create_binary_cols)\n",
    "test['USER_GENDER'] = test['USER_GENDER'].apply(create_binary_cols)\n",
    "\n",
    "# Is_purchase_paid_via_mpesa_send_money column convert:\n",
    "train['IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY'] = train['IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY'].apply(create_binary_cols)\n",
    "test['IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY'] = test['IS_PURCHASE_PAID_VIA_MPESA_SEND_MONEY'].apply(create_binary_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0Y0uKWguZ9n"
   },
   "outputs": [],
   "source": [
    "# Separate the features from the target in the training data\n",
    "X = train.drop([\"MERCHANT_CATEGORIZED_AS\"], axis=1)\n",
    "y = train[\"MERCHANT_CATEGORIZED_AS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(n_estimators = 575,\n",
    "                           depth = 6,\n",
    "                           learning_rate = 0.05,\n",
    "                           random_strength = 0.5,\n",
    "                           #eval_metric = 'AUC',\n",
    "                           od_type = \"Iter\",\n",
    "                           #l2_leaf_reg = 100,\n",
    "                           od_wait = 100,\n",
    "                           task_type=\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X, y,\n",
    "    cat_features = ['season'],\n",
    "    plot = True, \n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(importance, names ,model_type):\n",
    "    \n",
    "    import seaborn as sns\n",
    "    #Create arrays from feature importance and feature names\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "\n",
    "    #Define size of bar plot\n",
    "    plt.figure(figsize=(12, 25))\n",
    "    #Plot Searborn bar chart\n",
    "    sns.barplot(x=fi_df['feature_importance'].head(300), y=fi_df['feature_names'].head(300))\n",
    "    #Add chart labels\n",
    "    plt.title(model_type + 'FEATURE IMPORTANCE')\n",
    "    plt.xlabel('FEATURE IMPORTANCE')\n",
    "    plt.ylabel('FEATURE NAMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_feature_importance(model.feature_importances_,model.feature_names_, 'catboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = model.feature_importances_\n",
    "names = model.feature_names_\n",
    "\n",
    "#Create arrays from feature importance and feature names\n",
    "feature_importance = np.array(importance)\n",
    "feature_names = np.array(names)\n",
    "\n",
    "#Create a DataFrame using a Dictionary\n",
    "data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "fi_df = pd.DataFrame(data)\n",
    "\n",
    "#Sort the DataFrame in order decreasing feature importance\n",
    "fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping features based on feature_importances\n",
    "cols_to_drop = fi_df[fi_df['feature_importance'] == 0]['feature_names'].values.tolist()\n",
    "\n",
    "X.drop(columns = cols_to_drop, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(n_estimators = 575,\n",
    "                           depth = 6,\n",
    "                           learning_rate = 0.05,\n",
    "                           random_strength = 0.5,\n",
    "                           #eval_metric = 'AUC',\n",
    "                           od_type = \"Iter\",\n",
    "                           #l2_leaf_reg = 100,\n",
    "                           od_wait = 100,\n",
    "                           task_type=\"GPU\")\n",
    "\n",
    "model.fit(\n",
    "    X, y,\n",
    "    cat_features = ['season'],\n",
    "    plot = True, \n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(model.feature_importances_,model.feature_names_, 'catboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJnV9lsd2-s-"
   },
   "source": [
    "### Making submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.concat([test['Transaction_ID'].reset_index(drop= True), \n",
    "           pd.DataFrame(model.predict_proba(test[X.columns]), columns = model.classes_)], axis = 1)#[ss.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xway9nYc3s1m"
   },
   "source": [
    "Save results in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoLeuE4M3uSs"
   },
   "outputs": [],
   "source": [
    "# Create the submission csv file\n",
    "sub[ss.columns].to_csv(f'sub_{datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\")}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Alvinapp competition.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
